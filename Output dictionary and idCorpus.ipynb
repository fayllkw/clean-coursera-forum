{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"processed data\"\n",
    "FILE = \"english_only_question_and_answer_without_instructor.csv\"\n",
    "dat = pd.read_csv(os.getcwd() + '/' + DATA_DIR + '/' + FILE, encoding = 'ISO-8859-1')\n",
    "# dat\n",
    "posts = dat[dat['post_content_length']>3]['post_content']\n",
    "# posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NO_DOCUMENT = len(posts)\n",
    "NO_DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "en_stop = set(stopwords.words('english'))\n",
    "en_stop.add(\"'s\")\n",
    "en_stop.add(\"'re\")\n",
    "en_stop.add(\"'m\")\n",
    "en_stop.add(\"'ve\")\n",
    "en_stop.add(\"n't\")\n",
    "en_stop.add(\"would\")\n",
    "en_stop.add(\"really\")\n",
    "en_stop.add(\"may\")\n",
    "en_stop.add(\"even\")\n",
    "en_stop.add(\"also\")\n",
    "en_stop.add(\"us\")\n",
    "import string\n",
    "exclude = set(string.punctuation)\n",
    "exclude.add(\"--\")\n",
    "exclude.add(\"\\'\\'\")\n",
    "exclude.add(\"...\")\n",
    "exclude.add(\"``\")\n",
    "exclude.add(\"..\")\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag(tag):\n",
    "    tag_dict = {'N': 'n', 'J': 'a', 'R': 'r', 'V': 'v'}\n",
    "    try:\n",
    "        return tag_dict[tag[0]]\n",
    "    except KeyError:\n",
    "        return 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    tokenized_lower = word_tokenize(doc.lower()) \n",
    "    tokenized_lower = [word for word in tokenized_lower if len(word)>2]\n",
    "    stop_free = [word for word in tokenized_lower if word not in en_stop]\n",
    "    punc_free = [d for d in stop_free if d not in exclude]\n",
    "    tags = nltk.pos_tag(punc_free)\n",
    "    tags = [convert_tag(tag[1]) for tag in tags]\n",
    "    lemmatized = [WordNetLemmatizer().lemmatize(word, tag) for word,tag in zip(punc_free,tags)]\n",
    "    return lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "931"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_corpus = [clean(d) for d in corpus]\n",
    "len(normalized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word.dfs[id2word.token2id['.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# id2word.token2id['hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5116"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dictionary\n",
    "id2word = corpora.Dictionary(normalized_corpus)\n",
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1401"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create filtered bog representation\n",
    "id2word.filter_extremes(no_below=3, no_above=0.1)\n",
    "bog_corpus_filtered = [id2word.doc2bow(doc) for doc in normalized_corpus]\n",
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 21, -1, 9, -1, -1, -1, -1, 12, -1, -1, 32, -1, 28, 34, 31, 35, 5, 33, 35, 31, -1, 35, 31, -1, 20, 18, 11, 8, -1, 16, 24, -1, 2, 31, -1, 31, 30, -1, 22, 24, 2, 38, 6, 9, 35, 24, 7, -1, -1, 31, 29, 14, -1, 31, -1, 19, -1, 10, 27, 31, 1, 0, 23, 13, 32, 35, -1, 26, 15, 17, -1, 37, 25, 3, 36]\n"
     ]
    }
   ],
   "source": [
    "doc_wids = [id2word.doc2idx(doc) for doc in normalized_corpus]\n",
    "print(doc_wids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['4', '21', '9', '12', '32', '28', '34', '31', '35', '5', '33', '35', '31', '35', '31', '20', '18', '11', '8', '16', '24', '2', '31', '31', '30', '22', '24', '2', '38', '6', '9', '35', '24', '7', '31', '29', '14', '31', '19', '10', '27', '31', '1', '0', '23', '13', '32', '35', '26', '15', '17', '37', '25', '3', '36']\n"
     ]
    }
   ],
   "source": [
    "doc_wids = [[str(i) for i in doc if i!=-1] for doc in doc_wids]\n",
    "print(doc_wids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4 21 9 12 32 28 34 31 35 5 33 35 31 35 31 20 18 11 8 16 24 2 31 31 30 22 24 2 38 6 9 35 24 7 31 29 14 31 19 10 27 31 1 0 23 13 32 35 26 15 17 37 25 3 36'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_wids = [\" \".join(doc) for doc in doc_wids]\n",
    "doc_wids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_wids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output doc_wids\n",
    "fileObject = open(os.pardir + '/BTM-master/arvr/arvr_wids.txt', 'w')\n",
    "for doc in doc_wids:\n",
    "    fileObject.write(doc)\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output dictionary\n",
    "fileObject = open(os.pardir + '/BTM-master/arvr/arvr_dict.txt', 'w')\n",
    "for i in id2word.items():\n",
    "    fileObject.write(str(i[0])+'\\t'+i[1])\n",
    "    fileObject.write('\\n')\n",
    "fileObject.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
